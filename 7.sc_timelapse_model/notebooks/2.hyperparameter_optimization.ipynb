{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pathlib\n",
    "import pprint\n",
    "import sys\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import toml\n",
    "import torch\n",
    "from optuna.samplers import RandomSampler\n",
    "\n",
    "sys.path.append(\"../ML_utils/\")\n",
    "\n",
    "from create_optimized_model import optimized_model_create\n",
    "from extract_best_trial import extract_best_trial_params\n",
    "from objective_creation import objective_model_optimizer\n",
    "from parameter_set import parameter_set\n",
    "from parameters import Parameters\n",
    "\n",
    "try:\n",
    "    cfg = get_ipython().config\n",
    "    in_notebook = True\n",
    "except NameError:\n",
    "    in_notebook = False\n",
    "if in_notebook:\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sc_profile shape: (182804, 2376)\n",
      "sc_endpoint_profile shape: (11340, 368)\n",
      "data_split_df shape: (14926, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>data_split</th>\n",
       "      <th>data_x_or_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7440</td>\n",
       "      <td>train_gt</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7461</td>\n",
       "      <td>train_gt</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7463</td>\n",
       "      <td>train_gt</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7468</td>\n",
       "      <td>train_gt</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7479</td>\n",
       "      <td>train_gt</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index data_split data_x_or_y\n",
       "0   7440   train_gt           X\n",
       "1   7461   train_gt           X\n",
       "2   7463   train_gt           X\n",
       "3   7468   train_gt           X\n",
       "4   7479   train_gt           X"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the data\n",
    "sc_file_path = pathlib.Path(\"../results/cleaned_sc_profile.parquet\").resolve(\n",
    "    strict=True\n",
    ")\n",
    "sc_endpoint_file_path = pathlib.Path(\n",
    "    \"../results/cleaned_endpoint_sc_profile.parquet\"\n",
    ").resolve(strict=True)\n",
    "\n",
    "data_split_file_path = pathlib.Path(\"../results/data_splits.parquet\").resolve(\n",
    "    strict=True\n",
    ")\n",
    "\n",
    "sc_profile = pd.read_parquet(sc_file_path)\n",
    "sc_endpoint_profile = pd.read_parquet(sc_endpoint_file_path)\n",
    "data_split_df = pd.read_parquet(data_split_file_path)\n",
    "print(f\"sc_profile shape: {sc_profile.shape}\")\n",
    "print(f\"sc_endpoint_profile shape: {sc_endpoint_profile.shape}\")\n",
    "print(f\"data_split_df shape: {data_split_df.shape}\")\n",
    "data_split_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sc_profile shape after dropping NaN: (14237, 2376)\n",
      "sc_endpoint_profile shape after dropping NaN: (11136, 368)\n",
      "sc_endpoint_profile shape after selecting features: (11136, 26)\n"
     ]
    }
   ],
   "source": [
    "# keep only the last timepoint\n",
    "sc_profile[\"Metadata_Time\"] = sc_profile[\"Metadata_Time\"].astype(\"float64\")\n",
    "sc_profile = sc_profile[\n",
    "    sc_profile[\"Metadata_Time\"] == sc_profile[\"Metadata_Time\"].max()\n",
    "]\n",
    "# drop Na values\n",
    "sc_profile.dropna(inplace=True)\n",
    "print(f\"sc_profile shape after dropping NaN: {sc_profile.shape}\")\n",
    "sc_endpoint_profile.dropna(inplace=True)\n",
    "print(f\"sc_endpoint_profile shape after dropping NaN: {sc_endpoint_profile.shape}\")\n",
    "# hardcode the features that should exist in the y data\n",
    "# this will be replaced in the future by an arg or config passed through\n",
    "selected_y_features = [\"Cells_Intensity_MeanIntensityEdge_AnnexinV\"]\n",
    "metadata_y_features = [x for x in sc_endpoint_profile.columns if \"Metadata_\" in x]\n",
    "sc_endpoint_profile = sc_endpoint_profile[metadata_y_features + selected_y_features]\n",
    "print(\n",
    "    f\"sc_endpoint_profile shape after selecting features: {sc_endpoint_profile.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_data_splits_df = data_split_df.loc[data_split_df[\"data_x_or_y\"] == \"X\"]\n",
    "profile_data_splits_df\n",
    "endpoint_data_splits_df = data_split_df.loc[data_split_df[\"data_x_or_y\"] == \"y\"]\n",
    "endpoint_data_splits_df\n",
    "# replace the index with the index column\n",
    "profile_data_splits_df = profile_data_splits_df.set_index(\n",
    "    \"index\", drop=True, verify_integrity=True\n",
    ")\n",
    "endpoint_data_splits_df = endpoint_data_splits_df.set_index(\n",
    "    \"index\", drop=True, verify_integrity=True\n",
    ")\n",
    "# remove the index name from profile_data_splits_df\n",
    "profile_data_splits_df.index.name = None\n",
    "# remove the index name from endpoint_data_splits_df\n",
    "endpoint_data_splits_df.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only indexes from sc_profile that are in the train_gt split\n",
    "train_gt_X = sc_profile.loc[\n",
    "    sc_profile.index.isin(\n",
    "        profile_data_splits_df.loc[\n",
    "            profile_data_splits_df[\"data_split\"] == \"train_gt\"\n",
    "        ].index\n",
    "    )\n",
    "]\n",
    "val_gt_X = sc_profile.loc[\n",
    "    sc_profile.index.isin(\n",
    "        profile_data_splits_df.loc[\n",
    "            profile_data_splits_df[\"data_split\"] == \"val_gt\"\n",
    "        ].index\n",
    "    )\n",
    "]\n",
    "train_gt_y = sc_endpoint_profile.loc[\n",
    "    sc_endpoint_profile.index.isin(\n",
    "        endpoint_data_splits_df.loc[\n",
    "            endpoint_data_splits_df[\"data_split\"] == \"train_gt\"\n",
    "        ].index\n",
    "    )\n",
    "]\n",
    "val_gt_y = sc_endpoint_profile.loc[\n",
    "    sc_endpoint_profile.index.isin(\n",
    "        endpoint_data_splits_df.loc[\n",
    "            endpoint_data_splits_df[\"data_split\"] == \"val_gt\"\n",
    "        ].index\n",
    "    )\n",
    "]\n",
    "\n",
    "# assertion checks\n",
    "assert train_gt_X.shape[0] == train_gt_y.shape[0]\n",
    "assert val_gt_X.shape[0] == val_gt_y.shape[0]\n",
    "assert train_gt_X.shape[1] == val_gt_X.shape[1]\n",
    "assert train_gt_y.shape[1] == val_gt_y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1347139/735914267.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_gt_X.drop(columns=metadata_X_cols, inplace=True)\n",
      "/tmp/ipykernel_1347139/735914267.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_gt_y.drop(columns=metadata_y_cols, inplace=True)\n",
      "/tmp/ipykernel_1347139/735914267.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_gt_X.drop(columns=metadata_X_cols, inplace=True)\n",
      "/tmp/ipykernel_1347139/735914267.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_gt_y.drop(columns=metadata_y_cols, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# get metadata\n",
    "metadata_X_cols = [x for x in train_gt_X.columns if \"Metadata_\" in x]\n",
    "metadata_y_cols = [x for x in train_gt_y.columns if \"Metadata_\" in x]\n",
    "\n",
    "\n",
    "train_gt_X_metadata = train_gt_X[metadata_X_cols]\n",
    "train_gt_X.drop(columns=metadata_X_cols, inplace=True)\n",
    "train_gt_y_metadata = train_gt_y[metadata_y_cols]\n",
    "train_gt_y.drop(columns=metadata_y_cols, inplace=True)\n",
    "val_gt_X_metadata = val_gt_X[metadata_X_cols]\n",
    "val_gt_X.drop(columns=metadata_X_cols, inplace=True)\n",
    "val_gt_y_metadata = val_gt_y[metadata_y_cols]\n",
    "val_gt_y.drop(columns=metadata_y_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data\n",
    "shuffled_train_gt_X = train_gt_X.copy()\n",
    "for col in shuffled_train_gt_X.columns:\n",
    "    if col.startswith(\"Metadata_\"):\n",
    "        continue\n",
    "    shuffled_train_gt_X[col] = np.random.permutation(shuffled_train_gt_X[col].values)\n",
    "shuffled_val_gt_X = val_gt_X.copy()\n",
    "for col in shuffled_val_gt_X.columns:\n",
    "    if col.startswith(\"Metadata_\"):\n",
    "        continue\n",
    "    shuffled_val_gt_X[col] = np.random.permutation(shuffled_val_gt_X[col].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_features: 2338\n",
      "n_outputs: 1\n",
      "n_metadata_features: 38\n"
     ]
    }
   ],
   "source": [
    "# number of input features\n",
    "n_features = train_gt_X.shape[1]\n",
    "# number of output features\n",
    "n_outputs = train_gt_y.shape[1]\n",
    "# number of metadata features\n",
    "n_metadata_features = train_gt_X_metadata.shape[1]\n",
    "\n",
    "print(f\"n_features: {n_features}\")\n",
    "print(f\"n_outputs: {n_outputs}\")\n",
    "print(f\"n_metadata_features: {n_metadata_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = Parameters()\n",
    "ml_configs = toml.load(\"../ML_utils/regression_class_config.toml\")\n",
    "mlp_params = parameter_set(params, ml_configs)\n",
    "mlp_params.IN_FEATURES = n_features\n",
    "mlp_params.OUT_FEATURES = n_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(367, 2338) (367, 1)\n",
      "(49, 2338) (49, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_gt_X.shape, train_gt_y.shape)\n",
    "print(val_gt_X.shape, val_gt_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(train_gt_X.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(train_gt_y.values, dtype=torch.float32)\n",
    "X_val = torch.tensor(val_gt_X.values, dtype=torch.float32)\n",
    "y_val = torch.tensor(val_gt_y.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train dtypes: torch.float32\n",
      "y_train dtypes: torch.float32\n",
      "X_val dtypes: torch.float32\n",
      "y_val dtypes: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# get the dtypes of the data\n",
    "print(f\"X_train dtypes: {X_train.dtype}\")\n",
    "print(f\"y_train dtypes: {y_train.dtype}\")\n",
    "print(f\"X_val dtypes: {X_val.dtype}\")\n",
    "print(f\"y_val dtypes: {y_val.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce data objects for train, val and test datasets\n",
    "train_data = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "val_data = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "\n",
    "\n",
    "# convert data class into a dataloader to be compatible with pytorch\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_data, batch_size=mlp_params.HYPERPARAMETER_BATCH_SIZE, shuffle=True\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_data, batch_size=mlp_params.HYPERPARAMETER_BATCH_SIZE, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "pathlib.Path(\"../logs\").mkdir(parents=True, exist_ok=True)\n",
    "# Create a file handler\n",
    "file_handler = logging.FileHandler(\"../logs/optuna_log.txt\")\n",
    "file_handler.setLevel(logging.INFO)\n",
    "\n",
    "# Create a formatter and set it to the handler\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "# Add the handler to the logger\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Optional: Set Optuna to use this logger\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "optuna.logging.enable_propagation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-13 08:54:08,637] A new study created in memory with name: live_cell_AnnexinV_prediction\n",
      "[I 2025-05-13 08:54:09,514] Trial 0 finished with value: 0.6374997544288635 and parameters: {'n_layers': 1, 'n_units_l0': 4, 'dropout_0': 0.4660576954504388, 'learning_rate': 0.051391619561266456, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.6374997544288635.\n",
      "[I 2025-05-13 08:54:09,730] Trial 1 finished with value: 26.295696195364 and parameters: {'n_layers': 3, 'n_units_l0': 3, 'dropout_0': 0.3264689170479415, 'n_units_l1': 6, 'dropout_1': 0.25544510305757945, 'n_units_l2': 10, 'dropout_2': 0.7041916432577999, 'learning_rate': 0.03856842333097559, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.6374997544288635.\n",
      "[I 2025-05-13 08:54:09,913] Trial 2 finished with value: 845.9318025493621 and parameters: {'n_layers': 1, 'n_units_l0': 3, 'dropout_0': 0.7994234878697071, 'learning_rate': 0.08606655540890426, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.6374997544288635.\n",
      "[I 2025-05-13 08:54:10,146] Trial 3 finished with value: 0.5515957999229432 and parameters: {'n_layers': 1, 'n_units_l0': 3, 'dropout_0': 0.6158030019801688, 'learning_rate': 0.00938625140182581, 'optimizer': 'SGD'}. Best is trial 3 with value: 0.5515957999229432.\n",
      "[I 2025-05-13 08:54:10,394] Trial 4 finished with value: 0.564654815196991 and parameters: {'n_layers': 4, 'n_units_l0': 9, 'dropout_0': 0.5474363001152242, 'n_units_l1': 2, 'dropout_1': 0.6097166119494127, 'n_units_l2': 8, 'dropout_2': 0.07136818952190326, 'n_units_l3': 9, 'dropout_3': 0.07214801478436027, 'learning_rate': 0.05561873627590683, 'optimizer': 'SGD'}. Best is trial 3 with value: 0.5515957999229432.\n",
      "[I 2025-05-13 08:54:10,401] Trial 5 pruned. \n",
      "[I 2025-05-13 08:54:10,409] Trial 6 pruned. \n",
      "[I 2025-05-13 08:54:10,686] Trial 7 finished with value: 0.563194340467453 and parameters: {'n_layers': 4, 'n_units_l0': 4, 'dropout_0': 0.23548366536254267, 'n_units_l1': 3, 'dropout_1': 0.5526039340553494, 'n_units_l2': 9, 'dropout_2': 0.35830959548600566, 'n_units_l3': 2, 'dropout_3': 0.793818807827744, 'learning_rate': 0.06296300568975859, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.5515957999229432.\n",
      "[I 2025-05-13 08:54:10,695] Trial 8 pruned. \n",
      "[I 2025-05-13 08:54:11,001] Trial 9 finished with value: 0.6010481584072113 and parameters: {'n_layers': 3, 'n_units_l0': 9, 'dropout_0': 0.2723376157586034, 'n_units_l1': 6, 'dropout_1': 0.6040132134575592, 'n_units_l2': 9, 'dropout_2': 0.12281882730260633, 'learning_rate': 0.09531815995249991, 'optimizer': 'SGD'}. Best is trial 3 with value: 0.5515957999229432.\n",
      "[I 2025-05-13 08:54:11,194] Trial 10 finished with value: 0.5536681377887726 and parameters: {'n_layers': 4, 'n_units_l0': 2, 'dropout_0': 0.5230569320570458, 'n_units_l1': 2, 'dropout_1': 0.3689894423623807, 'n_units_l2': 6, 'dropout_2': 0.1301089888562858, 'n_units_l3': 10, 'dropout_3': 0.3011648998413217, 'learning_rate': 0.02490452238644817, 'optimizer': 'SGD'}. Best is trial 3 with value: 0.5515957999229432.\n",
      "[I 2025-05-13 08:54:11,200] Trial 11 pruned. \n",
      "[I 2025-05-13 08:54:11,206] Trial 12 pruned. \n",
      "[I 2025-05-13 08:54:11,385] Trial 13 finished with value: 0.5876206827163696 and parameters: {'n_layers': 1, 'n_units_l0': 3, 'dropout_0': 0.483966887687616, 'learning_rate': 0.0489423238391685, 'optimizer': 'SGD'}. Best is trial 3 with value: 0.5515957999229432.\n",
      "[I 2025-05-13 08:54:11,400] Trial 14 pruned. \n",
      "[I 2025-05-13 08:54:11,405] Trial 15 pruned. \n",
      "[I 2025-05-13 08:54:11,411] Trial 16 pruned. \n",
      "[I 2025-05-13 08:54:11,418] Trial 17 pruned. \n",
      "[I 2025-05-13 08:54:11,424] Trial 18 pruned. \n",
      "[I 2025-05-13 08:54:11,432] Trial 19 pruned. \n",
      "[I 2025-05-13 08:54:11,443] Trial 20 pruned. \n",
      "[I 2025-05-13 08:54:11,450] Trial 21 pruned. \n",
      "[I 2025-05-13 08:54:11,466] Trial 22 pruned. \n",
      "[I 2025-05-13 08:54:11,475] Trial 23 pruned. \n",
      "[I 2025-05-13 08:54:11,485] Trial 24 pruned. \n",
      "[I 2025-05-13 08:54:11,493] Trial 25 pruned. \n",
      "[I 2025-05-13 08:54:11,503] Trial 26 pruned. \n",
      "[I 2025-05-13 08:54:11,790] Trial 27 finished with value: 0.5593329858779907 and parameters: {'n_layers': 5, 'n_units_l0': 7, 'dropout_0': 0.7972522898026944, 'n_units_l1': 2, 'dropout_1': 0.606508805042908, 'n_units_l2': 8, 'dropout_2': 0.20650260831632283, 'n_units_l3': 6, 'dropout_3': 0.5054991750639832, 'n_units_l4': 9, 'dropout_4': 0.5702411952907629, 'learning_rate': 0.028821328303403126, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.5515957999229432.\n",
      "[I 2025-05-13 08:54:11,797] Trial 28 pruned. \n",
      "[I 2025-05-13 08:54:11,817] Trial 29 pruned. \n",
      "[I 2025-05-13 08:54:11,823] Trial 30 pruned. \n",
      "[I 2025-05-13 08:54:12,034] Trial 31 finished with value: 0.5515703392028809 and parameters: {'n_layers': 4, 'n_units_l0': 7, 'dropout_0': 0.27690460946889356, 'n_units_l1': 2, 'dropout_1': 0.6327031933734064, 'n_units_l2': 7, 'dropout_2': 0.6346453038511692, 'n_units_l3': 4, 'dropout_3': 0.21101445356768206, 'learning_rate': 0.0003568670730857824, 'optimizer': 'SGD'}. Best is trial 31 with value: 0.5515703392028809.\n",
      "[I 2025-05-13 08:54:12,247] Trial 32 finished with value: 0.5669288063049316 and parameters: {'n_layers': 5, 'n_units_l0': 5, 'dropout_0': 0.1323090986546659, 'n_units_l1': 5, 'dropout_1': 0.4490118449619877, 'n_units_l2': 7, 'dropout_2': 0.7229197933027973, 'n_units_l3': 10, 'dropout_3': 0.493106722833744, 'n_units_l4': 6, 'dropout_4': 0.5935474850334234, 'learning_rate': 0.04342768893688662, 'optimizer': 'SGD'}. Best is trial 31 with value: 0.5515703392028809.\n",
      "[I 2025-05-13 08:54:12,252] Trial 33 pruned. \n",
      "[I 2025-05-13 08:54:12,467] Trial 34 finished with value: 0.5496006619930267 and parameters: {'n_layers': 5, 'n_units_l0': 10, 'dropout_0': 0.27081221880147544, 'n_units_l1': 10, 'dropout_1': 0.2572438363671778, 'n_units_l2': 2, 'dropout_2': 0.3165459081412043, 'n_units_l3': 3, 'dropout_3': 0.6051727341659178, 'n_units_l4': 10, 'dropout_4': 0.09912774467614162, 'learning_rate': 0.006447615708453802, 'optimizer': 'SGD'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:12,473] Trial 35 pruned. \n",
      "[I 2025-05-13 08:54:12,686] Trial 36 finished with value: 0.5602301061153412 and parameters: {'n_layers': 5, 'n_units_l0': 7, 'dropout_0': 0.781348597053886, 'n_units_l1': 9, 'dropout_1': 0.4744932147779401, 'n_units_l2': 6, 'dropout_2': 0.17975527441400257, 'n_units_l3': 6, 'dropout_3': 0.5573595721761039, 'n_units_l4': 8, 'dropout_4': 0.2921264747298948, 'learning_rate': 0.08680060594761115, 'optimizer': 'SGD'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:12,692] Trial 37 pruned. \n",
      "[I 2025-05-13 08:54:12,698] Trial 38 pruned. \n",
      "[I 2025-05-13 08:54:12,706] Trial 39 pruned. \n",
      "[I 2025-05-13 08:54:12,712] Trial 40 pruned. \n",
      "[I 2025-05-13 08:54:12,719] Trial 41 pruned. \n",
      "[I 2025-05-13 08:54:12,726] Trial 42 pruned. \n",
      "[I 2025-05-13 08:54:12,733] Trial 43 pruned. \n",
      "[I 2025-05-13 08:54:12,738] Trial 44 pruned. \n",
      "[I 2025-05-13 08:54:12,973] Trial 45 finished with value: 0.5524151706695557 and parameters: {'n_layers': 4, 'n_units_l0': 4, 'dropout_0': 0.41583907074079857, 'n_units_l1': 9, 'dropout_1': 0.23659704481876032, 'n_units_l2': 4, 'dropout_2': 0.34675084691676183, 'n_units_l3': 5, 'dropout_3': 0.4805258826588522, 'learning_rate': 0.01927061178244189, 'optimizer': 'Adam'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:12,980] Trial 46 pruned. \n",
      "[I 2025-05-13 08:54:12,985] Trial 47 pruned. \n",
      "[I 2025-05-13 08:54:12,991] Trial 48 pruned. \n",
      "[I 2025-05-13 08:54:12,997] Trial 49 pruned. \n",
      "[I 2025-05-13 08:54:13,003] Trial 50 pruned. \n",
      "[I 2025-05-13 08:54:13,010] Trial 51 pruned. \n",
      "[I 2025-05-13 08:54:13,017] Trial 52 pruned. \n",
      "[I 2025-05-13 08:54:13,024] Trial 53 pruned. \n",
      "[I 2025-05-13 08:54:13,034] Trial 54 pruned. \n",
      "[I 2025-05-13 08:54:13,054] Trial 55 pruned. \n",
      "[I 2025-05-13 08:54:13,063] Trial 56 pruned. \n",
      "[I 2025-05-13 08:54:13,069] Trial 57 pruned. \n",
      "[I 2025-05-13 08:54:13,272] Trial 58 finished with value: 0.558793054819107 and parameters: {'n_layers': 2, 'n_units_l0': 4, 'dropout_0': 0.6461696184063398, 'n_units_l1': 2, 'dropout_1': 0.7967059973738048, 'learning_rate': 0.059584156759345475, 'optimizer': 'RMSprop'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:13,279] Trial 59 pruned. \n",
      "[I 2025-05-13 08:54:13,285] Trial 60 pruned. \n",
      "[I 2025-05-13 08:54:13,292] Trial 61 pruned. \n",
      "[I 2025-05-13 08:54:13,297] Trial 62 pruned. \n",
      "[I 2025-05-13 08:54:13,303] Trial 63 pruned. \n",
      "[I 2025-05-13 08:54:13,309] Trial 64 pruned. \n",
      "[I 2025-05-13 08:54:13,315] Trial 65 pruned. \n",
      "[I 2025-05-13 08:54:13,322] Trial 66 pruned. \n",
      "[I 2025-05-13 08:54:13,330] Trial 67 pruned. \n",
      "[I 2025-05-13 08:54:13,338] Trial 68 pruned. \n",
      "[I 2025-05-13 08:54:13,347] Trial 69 pruned. \n",
      "[I 2025-05-13 08:54:13,355] Trial 70 pruned. \n",
      "[I 2025-05-13 08:54:13,362] Trial 71 pruned. \n",
      "[I 2025-05-13 08:54:13,370] Trial 72 pruned. \n",
      "[I 2025-05-13 08:54:13,377] Trial 73 pruned. \n",
      "[I 2025-05-13 08:54:13,384] Trial 74 pruned. \n",
      "[I 2025-05-13 08:54:13,392] Trial 75 pruned. \n",
      "[I 2025-05-13 08:54:13,399] Trial 76 pruned. \n",
      "[I 2025-05-13 08:54:13,406] Trial 77 pruned. \n",
      "[I 2025-05-13 08:54:13,414] Trial 78 pruned. \n",
      "[I 2025-05-13 08:54:13,422] Trial 79 pruned. \n",
      "[I 2025-05-13 08:54:13,431] Trial 80 pruned. \n",
      "[I 2025-05-13 08:54:13,438] Trial 81 pruned. \n",
      "[I 2025-05-13 08:54:13,445] Trial 82 pruned. \n",
      "[I 2025-05-13 08:54:13,455] Trial 83 pruned. \n",
      "[I 2025-05-13 08:54:13,462] Trial 84 pruned. \n",
      "[I 2025-05-13 08:54:13,470] Trial 85 pruned. \n",
      "[I 2025-05-13 08:54:13,764] Trial 86 finished with value: 0.5607535326480866 and parameters: {'n_layers': 5, 'n_units_l0': 6, 'dropout_0': 0.5370091111331889, 'n_units_l1': 7, 'dropout_1': 0.6660934731553679, 'n_units_l2': 3, 'dropout_2': 0.704658902912609, 'n_units_l3': 8, 'dropout_3': 0.42381471743295146, 'n_units_l4': 7, 'dropout_4': 0.5436157841820775, 'learning_rate': 0.03184118096859063, 'optimizer': 'Adam'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:14,005] Trial 87 finished with value: 0.5610421931743622 and parameters: {'n_layers': 5, 'n_units_l0': 2, 'dropout_0': 0.4351603783700902, 'n_units_l1': 2, 'dropout_1': 0.7843468410223425, 'n_units_l2': 4, 'dropout_2': 0.4516063293537684, 'n_units_l3': 3, 'dropout_3': 0.20754236934113962, 'n_units_l4': 4, 'dropout_4': 0.07936012260270482, 'learning_rate': 0.09943719645008683, 'optimizer': 'Adam'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:14,295] Trial 88 finished with value: 0.5509181678295135 and parameters: {'n_layers': 4, 'n_units_l0': 3, 'dropout_0': 0.5005955947492089, 'n_units_l1': 3, 'dropout_1': 0.33496729430124833, 'n_units_l2': 9, 'dropout_2': 0.6151369163104142, 'n_units_l3': 10, 'dropout_3': 0.16456865066290177, 'learning_rate': 0.0409567835470037, 'optimizer': 'SGD'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:14,524] Trial 89 finished with value: 0.5599458682537078 and parameters: {'n_layers': 5, 'n_units_l0': 7, 'dropout_0': 0.39996159702815365, 'n_units_l1': 5, 'dropout_1': 0.6540959058831662, 'n_units_l2': 9, 'dropout_2': 0.603365115746917, 'n_units_l3': 10, 'dropout_3': 0.5585029298614066, 'n_units_l4': 4, 'dropout_4': 0.570981264782124, 'learning_rate': 0.028424783114101453, 'optimizer': 'Adam'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:14,529] Trial 90 pruned. \n",
      "[I 2025-05-13 08:54:14,535] Trial 91 pruned. \n",
      "[I 2025-05-13 08:54:14,541] Trial 92 pruned. \n",
      "[I 2025-05-13 08:54:14,548] Trial 93 pruned. \n",
      "[I 2025-05-13 08:54:14,554] Trial 94 pruned. \n",
      "[I 2025-05-13 08:54:14,560] Trial 95 pruned. \n",
      "[I 2025-05-13 08:54:14,566] Trial 96 pruned. \n",
      "[I 2025-05-13 08:54:14,574] Trial 97 pruned. \n",
      "[I 2025-05-13 08:54:14,796] Trial 98 finished with value: 0.5579237353801727 and parameters: {'n_layers': 4, 'n_units_l0': 7, 'dropout_0': 0.3517766315770931, 'n_units_l1': 5, 'dropout_1': 0.3991200664358368, 'n_units_l2': 7, 'dropout_2': 0.7527912897822584, 'n_units_l3': 2, 'dropout_3': 0.5480502553315757, 'learning_rate': 0.007998004487536656, 'optimizer': 'RMSprop'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:14,802] Trial 99 pruned. \n",
      "[I 2025-05-13 08:54:14,808] Trial 100 pruned. \n",
      "[I 2025-05-13 08:54:14,815] Trial 101 pruned. \n",
      "[I 2025-05-13 08:54:14,824] Trial 102 pruned. \n",
      "[I 2025-05-13 08:54:14,830] Trial 103 pruned. \n",
      "[I 2025-05-13 08:54:14,838] Trial 104 pruned. \n",
      "[I 2025-05-13 08:54:15,054] Trial 105 finished with value: 0.6252604484558105 and parameters: {'n_layers': 3, 'n_units_l0': 9, 'dropout_0': 0.2426661965587104, 'n_units_l1': 6, 'dropout_1': 0.17190583580495428, 'n_units_l2': 10, 'dropout_2': 0.425098764897363, 'learning_rate': 0.047945341950653836, 'optimizer': 'SGD'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:15,060] Trial 106 pruned. \n",
      "[I 2025-05-13 08:54:15,065] Trial 107 pruned. \n",
      "[I 2025-05-13 08:54:15,072] Trial 108 pruned. \n",
      "[I 2025-05-13 08:54:15,078] Trial 109 pruned. \n",
      "[I 2025-05-13 08:54:15,086] Trial 110 pruned. \n",
      "[I 2025-05-13 08:54:15,091] Trial 111 pruned. \n",
      "[I 2025-05-13 08:54:15,098] Trial 112 pruned. \n",
      "[I 2025-05-13 08:54:15,104] Trial 113 pruned. \n",
      "[I 2025-05-13 08:54:15,324] Trial 114 finished with value: 0.5576729214191437 and parameters: {'n_layers': 5, 'n_units_l0': 6, 'dropout_0': 0.5358197271110647, 'n_units_l1': 2, 'dropout_1': 0.10388613678274265, 'n_units_l2': 6, 'dropout_2': 0.21196883909031916, 'n_units_l3': 5, 'dropout_3': 0.44165462257153254, 'n_units_l4': 10, 'dropout_4': 0.6788012009923952, 'learning_rate': 0.07523187590878586, 'optimizer': 'SGD'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:15,331] Trial 115 pruned. \n",
      "[I 2025-05-13 08:54:15,338] Trial 116 pruned. \n",
      "[I 2025-05-13 08:54:15,555] Trial 117 finished with value: 0.5662996971607208 and parameters: {'n_layers': 3, 'n_units_l0': 7, 'dropout_0': 0.21409504084640607, 'n_units_l1': 4, 'dropout_1': 0.77108229694513, 'n_units_l2': 2, 'dropout_2': 0.6004758100424913, 'learning_rate': 0.07542347089749843, 'optimizer': 'SGD'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:15,561] Trial 118 pruned. \n",
      "[I 2025-05-13 08:54:15,567] Trial 119 pruned. \n",
      "[I 2025-05-13 08:54:15,806] Trial 120 finished with value: 0.5595217871665955 and parameters: {'n_layers': 4, 'n_units_l0': 4, 'dropout_0': 0.32124403451043176, 'n_units_l1': 6, 'dropout_1': 0.06960811520453795, 'n_units_l2': 3, 'dropout_2': 0.12551297186721433, 'n_units_l3': 10, 'dropout_3': 0.7396121512384434, 'learning_rate': 0.05771412933393089, 'optimizer': 'Adam'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:15,813] Trial 121 pruned. \n",
      "[I 2025-05-13 08:54:15,818] Trial 122 pruned. \n",
      "[I 2025-05-13 08:54:15,824] Trial 123 pruned. \n",
      "[I 2025-05-13 08:54:15,831] Trial 124 pruned. \n",
      "[I 2025-05-13 08:54:15,839] Trial 125 pruned. \n",
      "[I 2025-05-13 08:54:15,848] Trial 126 pruned. \n",
      "[I 2025-05-13 08:54:15,855] Trial 127 pruned. \n",
      "[I 2025-05-13 08:54:15,863] Trial 128 pruned. \n",
      "[I 2025-05-13 08:54:15,871] Trial 129 pruned. \n",
      "[I 2025-05-13 08:54:15,877] Trial 130 pruned. \n",
      "[I 2025-05-13 08:54:15,886] Trial 131 pruned. \n",
      "[I 2025-05-13 08:54:15,895] Trial 132 pruned. \n",
      "[I 2025-05-13 08:54:15,903] Trial 133 pruned. \n",
      "[I 2025-05-13 08:54:15,911] Trial 134 pruned. \n",
      "[I 2025-05-13 08:54:15,918] Trial 135 pruned. \n",
      "[I 2025-05-13 08:54:15,926] Trial 136 pruned. \n",
      "[I 2025-05-13 08:54:15,933] Trial 137 pruned. \n",
      "[I 2025-05-13 08:54:16,385] Trial 138 finished with value: 0.550927414894104 and parameters: {'n_layers': 5, 'n_units_l0': 7, 'dropout_0': 0.44410560764785617, 'n_units_l1': 2, 'dropout_1': 0.42127470580539605, 'n_units_l2': 6, 'dropout_2': 0.5687635778250795, 'n_units_l3': 3, 'dropout_3': 0.06677867015866513, 'n_units_l4': 7, 'dropout_4': 0.7485404785420098, 'learning_rate': 0.005683907008637026, 'optimizer': 'Adam'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:16,394] Trial 139 pruned. \n",
      "[I 2025-05-13 08:54:16,404] Trial 140 pruned. \n",
      "[I 2025-05-13 08:54:16,414] Trial 141 pruned. \n",
      "[I 2025-05-13 08:54:16,658] Trial 142 finished with value: 0.5651843810081482 and parameters: {'n_layers': 2, 'n_units_l0': 9, 'dropout_0': 0.6488157085131881, 'n_units_l1': 5, 'dropout_1': 0.6471655002251295, 'learning_rate': 0.03986122110816568, 'optimizer': 'SGD'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:16,665] Trial 143 pruned. \n",
      "[I 2025-05-13 08:54:16,671] Trial 144 pruned. \n",
      "[I 2025-05-13 08:54:16,678] Trial 145 pruned. \n",
      "[I 2025-05-13 08:54:16,685] Trial 146 pruned. \n",
      "[I 2025-05-13 08:54:16,693] Trial 147 pruned. \n",
      "[I 2025-05-13 08:54:16,701] Trial 148 pruned. \n",
      "[I 2025-05-13 08:54:16,708] Trial 149 pruned. \n",
      "[I 2025-05-13 08:54:16,715] Trial 150 pruned. \n",
      "[I 2025-05-13 08:54:16,721] Trial 151 pruned. \n",
      "[I 2025-05-13 08:54:16,727] Trial 152 pruned. \n",
      "[I 2025-05-13 08:54:16,733] Trial 153 pruned. \n",
      "[I 2025-05-13 08:54:16,740] Trial 154 pruned. \n",
      "[I 2025-05-13 08:54:16,749] Trial 155 pruned. \n",
      "[I 2025-05-13 08:54:16,757] Trial 156 pruned. \n",
      "[I 2025-05-13 08:54:16,763] Trial 157 pruned. \n",
      "[I 2025-05-13 08:54:16,770] Trial 158 pruned. \n",
      "[I 2025-05-13 08:54:16,777] Trial 159 pruned. \n",
      "[I 2025-05-13 08:54:16,783] Trial 160 pruned. \n",
      "[I 2025-05-13 08:54:16,792] Trial 161 pruned. \n",
      "[I 2025-05-13 08:54:16,798] Trial 162 pruned. \n",
      "[I 2025-05-13 08:54:16,805] Trial 163 pruned. \n",
      "[I 2025-05-13 08:54:16,811] Trial 164 pruned. \n",
      "[I 2025-05-13 08:54:16,820] Trial 165 pruned. \n",
      "[I 2025-05-13 08:54:16,826] Trial 166 pruned. \n",
      "[I 2025-05-13 08:54:16,834] Trial 167 pruned. \n",
      "[I 2025-05-13 08:54:16,842] Trial 168 pruned. \n",
      "[I 2025-05-13 08:54:16,850] Trial 169 pruned. \n",
      "[I 2025-05-13 08:54:16,859] Trial 170 pruned. \n",
      "[I 2025-05-13 08:54:16,868] Trial 171 pruned. \n",
      "[I 2025-05-13 08:54:16,877] Trial 172 pruned. \n",
      "[I 2025-05-13 08:54:16,888] Trial 173 pruned. \n",
      "[I 2025-05-13 08:54:16,899] Trial 174 pruned. \n",
      "[I 2025-05-13 08:54:16,908] Trial 175 pruned. \n",
      "[I 2025-05-13 08:54:16,920] Trial 176 pruned. \n",
      "[I 2025-05-13 08:54:16,930] Trial 177 pruned. \n",
      "[I 2025-05-13 08:54:16,939] Trial 178 pruned. \n",
      "[I 2025-05-13 08:54:16,946] Trial 179 pruned. \n",
      "[I 2025-05-13 08:54:16,956] Trial 180 pruned. \n",
      "[I 2025-05-13 08:54:16,966] Trial 181 pruned. \n",
      "[I 2025-05-13 08:54:16,975] Trial 182 pruned. \n",
      "[I 2025-05-13 08:54:16,984] Trial 183 pruned. \n",
      "[I 2025-05-13 08:54:17,202] Trial 184 finished with value: 0.5599049079418182 and parameters: {'n_layers': 4, 'n_units_l0': 2, 'dropout_0': 0.5860509881917917, 'n_units_l1': 4, 'dropout_1': 0.3353807669031272, 'n_units_l2': 2, 'dropout_2': 0.4763875844027622, 'n_units_l3': 3, 'dropout_3': 0.5236968433984595, 'learning_rate': 0.03280144046391408, 'optimizer': 'Adam'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:17,209] Trial 185 pruned. \n",
      "[I 2025-05-13 08:54:17,216] Trial 186 pruned. \n",
      "[I 2025-05-13 08:54:17,221] Trial 187 pruned. \n",
      "[I 2025-05-13 08:54:17,228] Trial 188 pruned. \n",
      "[I 2025-05-13 08:54:17,235] Trial 189 pruned. \n",
      "[I 2025-05-13 08:54:17,242] Trial 190 pruned. \n",
      "[I 2025-05-13 08:54:17,470] Trial 191 finished with value: 0.5582334518432617 and parameters: {'n_layers': 5, 'n_units_l0': 4, 'dropout_0': 0.6140168179657833, 'n_units_l1': 7, 'dropout_1': 0.7324894128424408, 'n_units_l2': 9, 'dropout_2': 0.06811289901067509, 'n_units_l3': 8, 'dropout_3': 0.057336818304728546, 'n_units_l4': 8, 'dropout_4': 0.35862261185445865, 'learning_rate': 0.020661736312905143, 'optimizer': 'Adam'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:17,709] Trial 192 finished with value: 0.5622338056564331 and parameters: {'n_layers': 4, 'n_units_l0': 10, 'dropout_0': 0.13938963116140557, 'n_units_l1': 5, 'dropout_1': 0.12930795413766222, 'n_units_l2': 6, 'dropout_2': 0.600390876986767, 'n_units_l3': 4, 'dropout_3': 0.745337448004024, 'learning_rate': 0.04850155761144995, 'optimizer': 'Adam'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:17,715] Trial 193 pruned. \n",
      "[I 2025-05-13 08:54:17,721] Trial 194 pruned. \n",
      "[I 2025-05-13 08:54:17,967] Trial 195 finished with value: 0.559025046825409 and parameters: {'n_layers': 4, 'n_units_l0': 4, 'dropout_0': 0.7708575420910075, 'n_units_l1': 3, 'dropout_1': 0.19228446057428644, 'n_units_l2': 5, 'dropout_2': 0.14299970754209013, 'n_units_l3': 2, 'dropout_3': 0.26291349206347464, 'learning_rate': 0.057251968917804166, 'optimizer': 'Adam'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:17,972] Trial 196 pruned. \n",
      "[I 2025-05-13 08:54:17,979] Trial 197 pruned. \n",
      "[I 2025-05-13 08:54:17,987] Trial 198 pruned. \n",
      "[I 2025-05-13 08:54:17,995] Trial 199 pruned. \n",
      "[I 2025-05-13 08:54:18,004] Trial 200 pruned. \n",
      "[I 2025-05-13 08:54:18,013] Trial 201 pruned. \n",
      "[I 2025-05-13 08:54:18,022] Trial 202 pruned. \n",
      "[I 2025-05-13 08:54:18,031] Trial 203 pruned. \n",
      "[I 2025-05-13 08:54:18,040] Trial 204 pruned. \n",
      "[I 2025-05-13 08:54:18,047] Trial 205 pruned. \n",
      "[I 2025-05-13 08:54:18,055] Trial 206 pruned. \n",
      "[I 2025-05-13 08:54:18,062] Trial 207 pruned. \n",
      "[I 2025-05-13 08:54:18,067] Trial 208 pruned. \n",
      "[I 2025-05-13 08:54:18,074] Trial 209 pruned. \n",
      "[I 2025-05-13 08:54:18,080] Trial 210 pruned. \n",
      "[I 2025-05-13 08:54:18,087] Trial 211 pruned. \n",
      "[I 2025-05-13 08:54:18,348] Trial 212 finished with value: 0.5942002439498901 and parameters: {'n_layers': 4, 'n_units_l0': 8, 'dropout_0': 0.3968679138656816, 'n_units_l1': 9, 'dropout_1': 0.5928650365838145, 'n_units_l2': 2, 'dropout_2': 0.7676426673103159, 'n_units_l3': 5, 'dropout_3': 0.21642418289739834, 'learning_rate': 0.09267544749101843, 'optimizer': 'Adam'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:18,529] Trial 213 finished with value: 0.6573309099674225 and parameters: {'n_layers': 1, 'n_units_l0': 4, 'dropout_0': 0.3258062314112151, 'learning_rate': 0.026451425938259474, 'optimizer': 'SGD'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:18,536] Trial 214 pruned. \n",
      "[I 2025-05-13 08:54:18,542] Trial 215 pruned. \n",
      "[I 2025-05-13 08:54:18,550] Trial 216 pruned. \n",
      "[I 2025-05-13 08:54:18,561] Trial 217 pruned. \n",
      "[I 2025-05-13 08:54:18,570] Trial 218 pruned. \n",
      "[I 2025-05-13 08:54:18,579] Trial 219 pruned. \n",
      "[I 2025-05-13 08:54:18,587] Trial 220 pruned. \n",
      "[I 2025-05-13 08:54:18,596] Trial 221 pruned. \n",
      "[I 2025-05-13 08:54:18,605] Trial 222 pruned. \n",
      "[I 2025-05-13 08:54:18,614] Trial 223 pruned. \n",
      "[I 2025-05-13 08:54:18,909] Trial 224 finished with value: 0.5557772171497345 and parameters: {'n_layers': 5, 'n_units_l0': 2, 'dropout_0': 0.6029757302158542, 'n_units_l1': 5, 'dropout_1': 0.2950779707165642, 'n_units_l2': 3, 'dropout_2': 0.3969956086201251, 'n_units_l3': 8, 'dropout_3': 0.666422596003736, 'n_units_l4': 5, 'dropout_4': 0.6969391748844973, 'learning_rate': 0.009790158590423809, 'optimizer': 'Adam'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:18,917] Trial 225 pruned. \n",
      "[I 2025-05-13 08:54:18,924] Trial 226 pruned. \n",
      "[I 2025-05-13 08:54:18,932] Trial 227 pruned. \n",
      "[I 2025-05-13 08:54:18,939] Trial 228 pruned. \n",
      "[I 2025-05-13 08:54:18,945] Trial 229 pruned. \n",
      "[I 2025-05-13 08:54:18,953] Trial 230 pruned. \n",
      "[I 2025-05-13 08:54:18,960] Trial 231 pruned. \n",
      "[I 2025-05-13 08:54:18,967] Trial 232 pruned. \n",
      "[I 2025-05-13 08:54:18,975] Trial 233 pruned. \n",
      "[I 2025-05-13 08:54:18,982] Trial 234 pruned. \n",
      "[I 2025-05-13 08:54:18,988] Trial 235 pruned. \n",
      "[I 2025-05-13 08:54:18,996] Trial 236 pruned. \n",
      "[I 2025-05-13 08:54:19,003] Trial 237 pruned. \n",
      "[I 2025-05-13 08:54:19,234] Trial 238 finished with value: 0.5587702918052674 and parameters: {'n_layers': 4, 'n_units_l0': 7, 'dropout_0': 0.7523049583819656, 'n_units_l1': 9, 'dropout_1': 0.6938959823837313, 'n_units_l2': 9, 'dropout_2': 0.4856538113738715, 'n_units_l3': 8, 'dropout_3': 0.7247951421600456, 'learning_rate': 0.06727632905041972, 'optimizer': 'Adam'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:19,240] Trial 239 pruned. \n",
      "[I 2025-05-13 08:54:19,247] Trial 240 pruned. \n",
      "[I 2025-05-13 08:54:19,254] Trial 241 pruned. \n",
      "[I 2025-05-13 08:54:19,260] Trial 242 pruned. \n",
      "[I 2025-05-13 08:54:19,268] Trial 243 pruned. \n",
      "[I 2025-05-13 08:54:19,277] Trial 244 pruned. \n",
      "[I 2025-05-13 08:54:19,284] Trial 245 pruned. \n",
      "[I 2025-05-13 08:54:19,290] Trial 246 pruned. \n",
      "[I 2025-05-13 08:54:19,296] Trial 247 pruned. \n",
      "[I 2025-05-13 08:54:19,304] Trial 248 pruned. \n",
      "[I 2025-05-13 08:54:19,311] Trial 249 pruned. \n",
      "[I 2025-05-13 08:54:19,319] Trial 250 pruned. \n",
      "[I 2025-05-13 08:54:19,327] Trial 251 pruned. \n",
      "[I 2025-05-13 08:54:19,336] Trial 252 pruned. \n",
      "[I 2025-05-13 08:54:19,343] Trial 253 pruned. \n",
      "[I 2025-05-13 08:54:19,350] Trial 254 pruned. \n",
      "[I 2025-05-13 08:54:19,357] Trial 255 pruned. \n",
      "[I 2025-05-13 08:54:19,366] Trial 256 pruned. \n",
      "[I 2025-05-13 08:54:19,374] Trial 257 pruned. \n",
      "[I 2025-05-13 08:54:19,380] Trial 258 pruned. \n",
      "[I 2025-05-13 08:54:19,389] Trial 259 pruned. \n",
      "[I 2025-05-13 08:54:19,395] Trial 260 pruned. \n",
      "[I 2025-05-13 08:54:19,403] Trial 261 pruned. \n",
      "[I 2025-05-13 08:54:19,410] Trial 262 pruned. \n",
      "[I 2025-05-13 08:54:19,420] Trial 263 pruned. \n",
      "[I 2025-05-13 08:54:19,430] Trial 264 pruned. \n",
      "[I 2025-05-13 08:54:19,437] Trial 265 pruned. \n",
      "[I 2025-05-13 08:54:19,445] Trial 266 pruned. \n",
      "[I 2025-05-13 08:54:19,454] Trial 267 pruned. \n",
      "[I 2025-05-13 08:54:19,463] Trial 268 pruned. \n",
      "[I 2025-05-13 08:54:19,472] Trial 269 pruned. \n",
      "[I 2025-05-13 08:54:19,481] Trial 270 pruned. \n",
      "[I 2025-05-13 08:54:19,489] Trial 271 pruned. \n",
      "[I 2025-05-13 08:54:19,497] Trial 272 pruned. \n",
      "[I 2025-05-13 08:54:19,505] Trial 273 pruned. \n",
      "[I 2025-05-13 08:54:19,512] Trial 274 pruned. \n",
      "[I 2025-05-13 08:54:19,742] Trial 275 finished with value: 0.5668582653999329 and parameters: {'n_layers': 4, 'n_units_l0': 6, 'dropout_0': 0.6005166400843006, 'n_units_l1': 10, 'dropout_1': 0.6424464536855155, 'n_units_l2': 9, 'dropout_2': 0.3704698871201206, 'n_units_l3': 4, 'dropout_3': 0.34931488273336153, 'learning_rate': 0.003397403352632899, 'optimizer': 'Adam'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:19,749] Trial 276 pruned. \n",
      "[I 2025-05-13 08:54:19,756] Trial 277 pruned. \n",
      "[I 2025-05-13 08:54:19,764] Trial 278 pruned. \n",
      "[I 2025-05-13 08:54:19,770] Trial 279 pruned. \n",
      "[I 2025-05-13 08:54:19,776] Trial 280 pruned. \n",
      "[I 2025-05-13 08:54:19,782] Trial 281 pruned. \n",
      "[I 2025-05-13 08:54:19,788] Trial 282 pruned. \n",
      "[I 2025-05-13 08:54:19,795] Trial 283 pruned. \n",
      "[I 2025-05-13 08:54:19,802] Trial 284 pruned. \n",
      "[I 2025-05-13 08:54:19,811] Trial 285 pruned. \n",
      "[I 2025-05-13 08:54:19,820] Trial 286 pruned. \n",
      "[I 2025-05-13 08:54:19,828] Trial 287 pruned. \n",
      "[I 2025-05-13 08:54:19,835] Trial 288 pruned. \n",
      "[I 2025-05-13 08:54:19,843] Trial 289 pruned. \n",
      "[I 2025-05-13 08:54:19,849] Trial 290 pruned. \n",
      "[I 2025-05-13 08:54:20,232] Trial 291 finished with value: 0.5570296621322632 and parameters: {'n_layers': 4, 'n_units_l0': 10, 'dropout_0': 0.25012615296764656, 'n_units_l1': 3, 'dropout_1': 0.1134302708526506, 'n_units_l2': 9, 'dropout_2': 0.7489792298332324, 'n_units_l3': 3, 'dropout_3': 0.3312671074294478, 'learning_rate': 0.0761141306617332, 'optimizer': 'SGD'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:20,240] Trial 292 pruned. \n",
      "[I 2025-05-13 08:54:20,248] Trial 293 pruned. \n",
      "[I 2025-05-13 08:54:20,256] Trial 294 pruned. \n",
      "[I 2025-05-13 08:54:20,262] Trial 295 pruned. \n",
      "[I 2025-05-13 08:54:20,269] Trial 296 pruned. \n",
      "[I 2025-05-13 08:54:20,276] Trial 297 pruned. \n",
      "[I 2025-05-13 08:54:20,283] Trial 298 pruned. \n",
      "[I 2025-05-13 08:54:20,291] Trial 299 pruned. \n",
      "[I 2025-05-13 08:54:20,298] Trial 300 pruned. \n",
      "[I 2025-05-13 08:54:20,305] Trial 301 pruned. \n",
      "[I 2025-05-13 08:54:20,313] Trial 302 pruned. \n",
      "[I 2025-05-13 08:54:20,323] Trial 303 pruned. \n",
      "[I 2025-05-13 08:54:20,333] Trial 304 pruned. \n",
      "[I 2025-05-13 08:54:20,342] Trial 305 pruned. \n",
      "[I 2025-05-13 08:54:20,348] Trial 306 pruned. \n",
      "[I 2025-05-13 08:54:20,357] Trial 307 pruned. \n",
      "[I 2025-05-13 08:54:20,365] Trial 308 pruned. \n",
      "[I 2025-05-13 08:54:20,373] Trial 309 pruned. \n",
      "[I 2025-05-13 08:54:20,382] Trial 310 pruned. \n",
      "[I 2025-05-13 08:54:20,392] Trial 311 pruned. \n",
      "[I 2025-05-13 08:54:20,402] Trial 312 pruned. \n",
      "[I 2025-05-13 08:54:20,413] Trial 313 pruned. \n",
      "[I 2025-05-13 08:54:20,434] Trial 314 pruned. \n",
      "[I 2025-05-13 08:54:20,447] Trial 315 pruned. \n",
      "[I 2025-05-13 08:54:20,462] Trial 316 pruned. \n",
      "[I 2025-05-13 08:54:20,472] Trial 317 pruned. \n",
      "[I 2025-05-13 08:54:20,481] Trial 318 pruned. \n",
      "[I 2025-05-13 08:54:20,489] Trial 319 pruned. \n",
      "[I 2025-05-13 08:54:20,830] Trial 320 finished with value: 0.5584046983718872 and parameters: {'n_layers': 4, 'n_units_l0': 10, 'dropout_0': 0.48131502795004466, 'n_units_l1': 9, 'dropout_1': 0.46954916204511427, 'n_units_l2': 9, 'dropout_2': 0.4698863788484214, 'n_units_l3': 4, 'dropout_3': 0.10346856273849576, 'learning_rate': 0.05891863461195835, 'optimizer': 'Adam'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:20,838] Trial 321 pruned. \n",
      "[I 2025-05-13 08:54:20,848] Trial 322 pruned. \n",
      "[I 2025-05-13 08:54:20,857] Trial 323 pruned. \n",
      "[I 2025-05-13 08:54:20,868] Trial 324 pruned. \n",
      "[I 2025-05-13 08:54:21,192] Trial 325 finished with value: 0.5591197574138641 and parameters: {'n_layers': 4, 'n_units_l0': 10, 'dropout_0': 0.6871360505712502, 'n_units_l1': 7, 'dropout_1': 0.6589957906395031, 'n_units_l2': 5, 'dropout_2': 0.13339966372401757, 'n_units_l3': 10, 'dropout_3': 0.20677189324387796, 'learning_rate': 0.027222993270621325, 'optimizer': 'Adam'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:21,199] Trial 326 pruned. \n",
      "[I 2025-05-13 08:54:21,208] Trial 327 pruned. \n",
      "[I 2025-05-13 08:54:21,218] Trial 328 pruned. \n",
      "[I 2025-05-13 08:54:21,224] Trial 329 pruned. \n",
      "[I 2025-05-13 08:54:21,231] Trial 330 pruned. \n",
      "[I 2025-05-13 08:54:21,239] Trial 331 pruned. \n",
      "[I 2025-05-13 08:54:21,245] Trial 332 pruned. \n",
      "[I 2025-05-13 08:54:21,253] Trial 333 pruned. \n",
      "[I 2025-05-13 08:54:21,260] Trial 334 pruned. \n",
      "[I 2025-05-13 08:54:21,530] Trial 335 finished with value: 0.5531814157962799 and parameters: {'n_layers': 4, 'n_units_l0': 9, 'dropout_0': 0.32484235140873446, 'n_units_l1': 4, 'dropout_1': 0.12112018772028486, 'n_units_l2': 10, 'dropout_2': 0.6179701469347503, 'n_units_l3': 9, 'dropout_3': 0.6313412352085995, 'learning_rate': 0.08022355380794208, 'optimizer': 'SGD'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:21,539] Trial 336 pruned. \n",
      "[I 2025-05-13 08:54:21,547] Trial 337 pruned. \n",
      "[I 2025-05-13 08:54:21,554] Trial 338 pruned. \n",
      "[I 2025-05-13 08:54:21,562] Trial 339 pruned. \n",
      "[I 2025-05-13 08:54:21,569] Trial 340 pruned. \n",
      "[I 2025-05-13 08:54:21,577] Trial 341 pruned. \n",
      "[I 2025-05-13 08:54:21,796] Trial 342 finished with value: 0.6577994120121002 and parameters: {'n_layers': 1, 'n_units_l0': 5, 'dropout_0': 0.6222128179653768, 'learning_rate': 0.08389219085775812, 'optimizer': 'SGD'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:21,805] Trial 343 pruned. \n",
      "[I 2025-05-13 08:54:21,812] Trial 344 pruned. \n",
      "[I 2025-05-13 08:54:22,060] Trial 345 finished with value: 0.5606855499744415 and parameters: {'n_layers': 4, 'n_units_l0': 4, 'dropout_0': 0.43133892950394875, 'n_units_l1': 3, 'dropout_1': 0.5733517026863462, 'n_units_l2': 6, 'dropout_2': 0.4524767109692922, 'n_units_l3': 10, 'dropout_3': 0.10282652759864468, 'learning_rate': 0.05040350912494667, 'optimizer': 'Adam'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:22,387] Trial 346 finished with value: 0.7280380094051361 and parameters: {'n_layers': 1, 'n_units_l0': 8, 'dropout_0': 0.4442980641436945, 'learning_rate': 0.043123127012166904, 'optimizer': 'SGD'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:22,604] Trial 347 finished with value: 0.5693238139152527 and parameters: {'n_layers': 1, 'n_units_l0': 5, 'dropout_0': 0.7415261582437351, 'learning_rate': 0.06540177826586958, 'optimizer': 'SGD'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:22,612] Trial 348 pruned. \n",
      "[I 2025-05-13 08:54:22,623] Trial 349 pruned. \n",
      "[I 2025-05-13 08:54:22,633] Trial 350 pruned. \n",
      "[I 2025-05-13 08:54:22,641] Trial 351 pruned. \n",
      "[I 2025-05-13 08:54:22,650] Trial 352 pruned. \n",
      "[I 2025-05-13 08:54:22,659] Trial 353 pruned. \n",
      "[I 2025-05-13 08:54:22,668] Trial 354 pruned. \n",
      "[I 2025-05-13 08:54:22,676] Trial 355 pruned. \n",
      "[I 2025-05-13 08:54:22,685] Trial 356 pruned. \n",
      "[I 2025-05-13 08:54:22,694] Trial 357 pruned. \n",
      "[I 2025-05-13 08:54:22,704] Trial 358 pruned. \n",
      "[I 2025-05-13 08:54:22,713] Trial 359 pruned. \n",
      "[I 2025-05-13 08:54:22,720] Trial 360 pruned. \n",
      "[I 2025-05-13 08:54:22,994] Trial 361 finished with value: 0.5606923282146454 and parameters: {'n_layers': 5, 'n_units_l0': 5, 'dropout_0': 0.33708056808909115, 'n_units_l1': 7, 'dropout_1': 0.6935314957422714, 'n_units_l2': 6, 'dropout_2': 0.5488951567703205, 'n_units_l3': 3, 'dropout_3': 0.07872082380669183, 'n_units_l4': 6, 'dropout_4': 0.21111578962666466, 'learning_rate': 0.06786675234255246, 'optimizer': 'Adam'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:23,001] Trial 362 pruned. \n",
      "[I 2025-05-13 08:54:23,006] Trial 363 pruned. \n",
      "[I 2025-05-13 08:54:23,014] Trial 364 pruned. \n",
      "[I 2025-05-13 08:54:23,020] Trial 365 pruned. \n",
      "[I 2025-05-13 08:54:23,029] Trial 366 pruned. \n",
      "[I 2025-05-13 08:54:23,037] Trial 367 pruned. \n",
      "[I 2025-05-13 08:54:23,045] Trial 368 pruned. \n",
      "[I 2025-05-13 08:54:23,053] Trial 369 pruned. \n",
      "[I 2025-05-13 08:54:23,059] Trial 370 pruned. \n",
      "[I 2025-05-13 08:54:23,280] Trial 371 finished with value: 0.5619304972887039 and parameters: {'n_layers': 3, 'n_units_l0': 2, 'dropout_0': 0.7964604568009446, 'n_units_l1': 2, 'dropout_1': 0.5100367611144568, 'n_units_l2': 7, 'dropout_2': 0.7214687790165655, 'learning_rate': 0.048054290768484595, 'optimizer': 'Adam'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:23,286] Trial 372 pruned. \n",
      "[I 2025-05-13 08:54:23,292] Trial 373 pruned. \n",
      "[I 2025-05-13 08:54:23,298] Trial 374 pruned. \n",
      "[I 2025-05-13 08:54:23,306] Trial 375 pruned. \n",
      "[I 2025-05-13 08:54:23,312] Trial 376 pruned. \n",
      "[I 2025-05-13 08:54:23,319] Trial 377 pruned. \n",
      "[I 2025-05-13 08:54:23,326] Trial 378 pruned. \n",
      "[I 2025-05-13 08:54:23,333] Trial 379 pruned. \n",
      "[I 2025-05-13 08:54:23,341] Trial 380 pruned. \n",
      "[I 2025-05-13 08:54:23,349] Trial 381 pruned. \n",
      "[I 2025-05-13 08:54:23,356] Trial 382 pruned. \n",
      "[I 2025-05-13 08:54:23,365] Trial 383 pruned. \n",
      "[I 2025-05-13 08:54:23,373] Trial 384 pruned. \n",
      "[I 2025-05-13 08:54:23,381] Trial 385 pruned. \n",
      "[I 2025-05-13 08:54:23,389] Trial 386 pruned. \n",
      "[I 2025-05-13 08:54:23,396] Trial 387 pruned. \n",
      "[I 2025-05-13 08:54:23,404] Trial 388 pruned. \n",
      "[I 2025-05-13 08:54:23,412] Trial 389 pruned. \n",
      "[I 2025-05-13 08:54:23,419] Trial 390 pruned. \n",
      "[I 2025-05-13 08:54:23,426] Trial 391 pruned. \n",
      "[I 2025-05-13 08:54:23,434] Trial 392 pruned. \n",
      "[I 2025-05-13 08:54:23,442] Trial 393 pruned. \n",
      "[I 2025-05-13 08:54:23,450] Trial 394 pruned. \n",
      "[I 2025-05-13 08:54:23,459] Trial 395 pruned. \n",
      "[I 2025-05-13 08:54:23,467] Trial 396 pruned. \n",
      "[I 2025-05-13 08:54:23,473] Trial 397 pruned. \n",
      "[I 2025-05-13 08:54:23,481] Trial 398 pruned. \n",
      "[I 2025-05-13 08:54:23,489] Trial 399 pruned. \n",
      "[I 2025-05-13 08:54:23,498] Trial 400 pruned. \n",
      "[I 2025-05-13 08:54:23,508] Trial 401 pruned. \n",
      "[I 2025-05-13 08:54:23,518] Trial 402 pruned. \n",
      "[I 2025-05-13 08:54:23,526] Trial 403 pruned. \n",
      "[I 2025-05-13 08:54:23,534] Trial 404 pruned. \n",
      "[I 2025-05-13 08:54:23,543] Trial 405 pruned. \n",
      "[I 2025-05-13 08:54:23,550] Trial 406 pruned. \n",
      "[I 2025-05-13 08:54:23,557] Trial 407 pruned. \n",
      "[I 2025-05-13 08:54:23,564] Trial 408 pruned. \n",
      "[I 2025-05-13 08:54:23,571] Trial 409 pruned. \n",
      "[I 2025-05-13 08:54:23,580] Trial 410 pruned. \n",
      "[I 2025-05-13 08:54:23,587] Trial 411 pruned. \n",
      "[I 2025-05-13 08:54:23,593] Trial 412 pruned. \n",
      "[I 2025-05-13 08:54:23,601] Trial 413 pruned. \n",
      "[I 2025-05-13 08:54:23,608] Trial 414 pruned. \n",
      "[I 2025-05-13 08:54:23,614] Trial 415 pruned. \n",
      "[I 2025-05-13 08:54:23,621] Trial 416 pruned. \n",
      "[I 2025-05-13 08:54:23,629] Trial 417 pruned. \n",
      "[I 2025-05-13 08:54:23,636] Trial 418 pruned. \n",
      "[I 2025-05-13 08:54:23,642] Trial 419 pruned. \n",
      "[I 2025-05-13 08:54:23,650] Trial 420 pruned. \n",
      "[I 2025-05-13 08:54:23,659] Trial 421 pruned. \n",
      "[I 2025-05-13 08:54:23,665] Trial 422 pruned. \n",
      "[I 2025-05-13 08:54:23,671] Trial 423 pruned. \n",
      "[I 2025-05-13 08:54:23,679] Trial 424 pruned. \n",
      "[I 2025-05-13 08:54:23,685] Trial 425 pruned. \n",
      "[I 2025-05-13 08:54:23,692] Trial 426 pruned. \n",
      "[I 2025-05-13 08:54:23,698] Trial 427 pruned. \n",
      "[I 2025-05-13 08:54:23,705] Trial 428 pruned. \n",
      "[I 2025-05-13 08:54:23,712] Trial 429 pruned. \n",
      "[I 2025-05-13 08:54:23,947] Trial 430 finished with value: 0.5736851704120636 and parameters: {'n_layers': 3, 'n_units_l0': 2, 'dropout_0': 0.5421110740109747, 'n_units_l1': 8, 'dropout_1': 0.31404971570581686, 'n_units_l2': 6, 'dropout_2': 0.17349990048300684, 'learning_rate': 0.04149763307947433, 'optimizer': 'SGD'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:23,956] Trial 431 pruned. \n",
      "[I 2025-05-13 08:54:23,965] Trial 432 pruned. \n",
      "[I 2025-05-13 08:54:23,974] Trial 433 pruned. \n",
      "[I 2025-05-13 08:54:23,982] Trial 434 pruned. \n",
      "[I 2025-05-13 08:54:23,990] Trial 435 pruned. \n",
      "[I 2025-05-13 08:54:23,998] Trial 436 pruned. \n",
      "[I 2025-05-13 08:54:24,006] Trial 437 pruned. \n",
      "[I 2025-05-13 08:54:24,017] Trial 438 pruned. \n",
      "[I 2025-05-13 08:54:24,025] Trial 439 pruned. \n",
      "[I 2025-05-13 08:54:24,034] Trial 440 pruned. \n",
      "[I 2025-05-13 08:54:24,044] Trial 441 pruned. \n",
      "[I 2025-05-13 08:54:24,052] Trial 442 pruned. \n",
      "[I 2025-05-13 08:54:24,062] Trial 443 pruned. \n",
      "[I 2025-05-13 08:54:24,071] Trial 444 pruned. \n",
      "[I 2025-05-13 08:54:24,079] Trial 445 pruned. \n",
      "[I 2025-05-13 08:54:24,087] Trial 446 pruned. \n",
      "[I 2025-05-13 08:54:24,099] Trial 447 pruned. \n",
      "[I 2025-05-13 08:54:24,108] Trial 448 pruned. \n",
      "[I 2025-05-13 08:54:24,118] Trial 449 pruned. \n",
      "[I 2025-05-13 08:54:24,132] Trial 450 pruned. \n",
      "[I 2025-05-13 08:54:24,141] Trial 451 pruned. \n",
      "[I 2025-05-13 08:54:24,151] Trial 452 pruned. \n",
      "[I 2025-05-13 08:54:24,164] Trial 453 pruned. \n",
      "[I 2025-05-13 08:54:24,184] Trial 454 pruned. \n",
      "[I 2025-05-13 08:54:24,194] Trial 455 pruned. \n",
      "[I 2025-05-13 08:54:24,203] Trial 456 pruned. \n",
      "[I 2025-05-13 08:54:24,210] Trial 457 pruned. \n",
      "[I 2025-05-13 08:54:24,217] Trial 458 pruned. \n",
      "[I 2025-05-13 08:54:24,223] Trial 459 pruned. \n",
      "[I 2025-05-13 08:54:24,232] Trial 460 pruned. \n",
      "[I 2025-05-13 08:54:24,243] Trial 461 pruned. \n",
      "[I 2025-05-13 08:54:24,252] Trial 462 pruned. \n",
      "[I 2025-05-13 08:54:24,259] Trial 463 pruned. \n",
      "[I 2025-05-13 08:54:24,267] Trial 464 pruned. \n",
      "[I 2025-05-13 08:54:24,275] Trial 465 pruned. \n",
      "[I 2025-05-13 08:54:24,281] Trial 466 pruned. \n",
      "[I 2025-05-13 08:54:24,289] Trial 467 pruned. \n",
      "[I 2025-05-13 08:54:24,294] Trial 468 pruned. \n",
      "[I 2025-05-13 08:54:24,301] Trial 469 pruned. \n",
      "[I 2025-05-13 08:54:24,309] Trial 470 pruned. \n",
      "[I 2025-05-13 08:54:24,316] Trial 471 pruned. \n",
      "[I 2025-05-13 08:54:24,326] Trial 472 pruned. \n",
      "[I 2025-05-13 08:54:24,335] Trial 473 pruned. \n",
      "[I 2025-05-13 08:54:24,344] Trial 474 pruned. \n",
      "[I 2025-05-13 08:54:24,352] Trial 475 pruned. \n",
      "[I 2025-05-13 08:54:24,360] Trial 476 pruned. \n",
      "[I 2025-05-13 08:54:24,368] Trial 477 pruned. \n",
      "[I 2025-05-13 08:54:24,376] Trial 478 pruned. \n",
      "[I 2025-05-13 08:54:24,384] Trial 479 pruned. \n",
      "[I 2025-05-13 08:54:24,391] Trial 480 pruned. \n",
      "[I 2025-05-13 08:54:24,398] Trial 481 pruned. \n",
      "[I 2025-05-13 08:54:24,404] Trial 482 pruned. \n",
      "[I 2025-05-13 08:54:24,411] Trial 483 pruned. \n",
      "[I 2025-05-13 08:54:24,419] Trial 484 pruned. \n",
      "[I 2025-05-13 08:54:24,426] Trial 485 pruned. \n",
      "[I 2025-05-13 08:54:24,433] Trial 486 pruned. \n",
      "[I 2025-05-13 08:54:24,441] Trial 487 pruned. \n",
      "[I 2025-05-13 08:54:24,447] Trial 488 pruned. \n",
      "[I 2025-05-13 08:54:24,456] Trial 489 pruned. \n",
      "[I 2025-05-13 08:54:24,465] Trial 490 pruned. \n",
      "[I 2025-05-13 08:54:24,475] Trial 491 pruned. \n",
      "[I 2025-05-13 08:54:24,484] Trial 492 pruned. \n",
      "[I 2025-05-13 08:54:24,492] Trial 493 pruned. \n",
      "[I 2025-05-13 08:54:24,721] Trial 494 finished with value: 0.8294108057022095 and parameters: {'n_layers': 2, 'n_units_l0': 2, 'dropout_0': 0.11631914511810797, 'n_units_l1': 7, 'dropout_1': 0.0592891066425479, 'learning_rate': 0.04717283005090329, 'optimizer': 'SGD'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:24,956] Trial 495 finished with value: 0.5565420472621918 and parameters: {'n_layers': 3, 'n_units_l0': 4, 'dropout_0': 0.3770278589652128, 'n_units_l1': 9, 'dropout_1': 0.10952199422101032, 'n_units_l2': 7, 'dropout_2': 0.3401906786126248, 'learning_rate': 0.03692376276029404, 'optimizer': 'SGD'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:25,196] Trial 496 finished with value: 0.5740579807758331 and parameters: {'n_layers': 3, 'n_units_l0': 5, 'dropout_0': 0.17039765451614358, 'n_units_l1': 3, 'dropout_1': 0.25830210118778424, 'n_units_l2': 7, 'dropout_2': 0.6285080869722297, 'learning_rate': 0.069062979731485, 'optimizer': 'SGD'}. Best is trial 34 with value: 0.5496006619930267.\n",
      "[I 2025-05-13 08:54:25,205] Trial 497 pruned. \n",
      "[I 2025-05-13 08:54:25,211] Trial 498 pruned. \n",
      "[I 2025-05-13 08:54:25,218] Trial 499 pruned. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5476478981971741"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wrap the objective function inside of a lambda function to pass args...\n",
    "objective_lambda_func = lambda trial: objective_model_optimizer(\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    trial=trial,\n",
    "    params=params,\n",
    "    metric=mlp_params.METRIC,\n",
    "    return_info=False,\n",
    ")\n",
    "# Study is the object for model optimization\n",
    "study = optuna.create_study(\n",
    "    direction=f\"{mlp_params.DIRECTION}\",\n",
    "    sampler=RandomSampler(),\n",
    "    study_name=\"live_cell_AnnexinV_prediction\",\n",
    ")\n",
    "# Here I apply the optimize function of the study to the objective function\n",
    "# This optimizes each parameter specified to be optimized from the defined search space\n",
    "study.optimize(objective_lambda_func, n_trials=mlp_params.N_TRIALS)\n",
    "# Prints out the best trial's optimized parameters\n",
    "objective_model_optimizer(\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    trial=study.best_trial,\n",
    "    params=params,\n",
    "    metric=mlp_params.METRIC,\n",
    "    return_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Cells_Intensity_MeanIntensityEdge_AnnexinV\"\n",
    "param_dict = extract_best_trial_params(study.best_params, params, model_name=model_name)\n",
    "\n",
    "\n",
    "untrained_model_archetecture_only = optimized_model_create(\n",
    "    params=params,\n",
    "    model_name=model_name,\n",
    ")\n",
    "# save the blank model architecture\n",
    "model_path = f\"../models/{model_name}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "markers",
         "name": "Objective Value",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          7,
          9,
          10,
          13,
          27,
          31,
          32,
          34,
          36,
          45,
          58,
          86,
          87,
          88,
          89,
          98,
          105,
          114,
          117,
          120,
          138,
          142,
          184,
          191,
          192,
          195,
          212,
          213,
          224,
          238,
          275,
          291,
          320,
          325,
          335,
          342,
          345,
          346,
          347,
          361,
          371,
          430,
          494,
          495,
          496
         ],
         "y": [
          0.6374997544288635,
          26.295696195364,
          845.9318025493621,
          0.5515957999229432,
          0.564654815196991,
          0.563194340467453,
          0.6010481584072113,
          0.5536681377887726,
          0.5876206827163696,
          0.5593329858779907,
          0.5515703392028809,
          0.5669288063049316,
          0.5496006619930267,
          0.5602301061153412,
          0.5524151706695557,
          0.558793054819107,
          0.5607535326480866,
          0.5610421931743622,
          0.5509181678295135,
          0.5599458682537078,
          0.5579237353801727,
          0.6252604484558105,
          0.5576729214191437,
          0.5662996971607208,
          0.5595217871665955,
          0.550927414894104,
          0.5651843810081482,
          0.5599049079418182,
          0.5582334518432617,
          0.5622338056564331,
          0.559025046825409,
          0.5942002439498901,
          0.6573309099674225,
          0.5557772171497345,
          0.5587702918052674,
          0.5668582653999329,
          0.5570296621322632,
          0.5584046983718872,
          0.5591197574138641,
          0.5531814157962799,
          0.6577994120121002,
          0.5606855499744415,
          0.7280380094051361,
          0.5693238139152527,
          0.5606923282146454,
          0.5619304972887039,
          0.5736851704120636,
          0.8294108057022095,
          0.5565420472621918,
          0.5740579807758331
         ]
        },
        {
         "mode": "lines",
         "name": "Best Value",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499
         ],
         "y": [
          0.6374997544288635,
          0.6374997544288635,
          0.6374997544288635,
          0.5515957999229432,
          0.5515957999229432,
          0.5515957999229432,
          0.5515957999229432,
          0.5515957999229432,
          0.5515957999229432,
          0.5515957999229432,
          0.5515957999229432,
          0.5515957999229432,
          0.5515957999229432,
          0.5515957999229432,
          0.5515957999229432,
          0.5515957999229432,
          0.5515957999229432,
          0.5515957999229432,
          0.5515957999229432,
          0.5515957999229432,
          0.5515957999229432,
          0.5515957999229432,
          0.5515957999229432,
          0.5515957999229432,
          0.5515957999229432,
          0.5515957999229432,
          0.5515957999229432,
          0.5515957999229432,
          0.5515957999229432,
          0.5515957999229432,
          0.5515957999229432,
          0.5515703392028809,
          0.5515703392028809,
          0.5515703392028809,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267,
          0.5496006619930267
         ]
        },
        {
         "marker": {
          "color": "#cccccc"
         },
         "mode": "markers",
         "name": "Infeasible Trial",
         "showlegend": false,
         "type": "scatter",
         "x": [],
         "y": []
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Optimization History Plot"
        },
        "xaxis": {
         "title": {
          "text": "Trial"
         }
        },
        "yaxis": {
         "title": {
          "text": "Objective Value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import optuna.visualization as vis\n",
    "\n",
    "# Assuming `study` is your Optuna study object\n",
    "fig = vis.plot_optimization_history(study)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timelapse_analysis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
