{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6948e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import warnings\n",
    "from typing import List, Tuple\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.metrics import (\n",
    "    explained_variance_score,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    r2_score,\n",
    ")\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3985ba8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Shuffle the data in the DataFrame.\n",
    "    \"\"\"\n",
    "    df_shuffled = df.copy()\n",
    "    for col in df_shuffled.columns:\n",
    "        # permute the columns\n",
    "        df_shuffled[col] = np.random.permutation(df_shuffled[col])\n",
    "    return df_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee95834e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sc_profile shape: (185502, 2383)\n",
      "sc_endpoint_profile shape: (4733, 545)\n"
     ]
    }
   ],
   "source": [
    "# read in the data\n",
    "sc_file_path = pathlib.Path(\"../results/cleaned_sc_profile.parquet\").resolve(\n",
    "    strict=True\n",
    ")\n",
    "sc_endpoint_file_path = pathlib.Path(\n",
    "    \"../results/cleaned_endpoint_sc_profile.parquet\"\n",
    ").resolve(strict=True)\n",
    "\n",
    "train_test_wells_file_path = pathlib.Path(\n",
    "    \"../../5.bulk_timelapse_model/data_splits/train_test_wells.parquet\"\n",
    ").resolve(strict=True)\n",
    "model_dir = pathlib.Path(\"../models\").resolve()\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_dir = pathlib.Path(\"../results\").resolve()\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "sc_profile = pd.read_parquet(sc_file_path)\n",
    "sc_endpoint_profile = pd.read_parquet(sc_endpoint_file_path)\n",
    "train_test_wells = pd.read_parquet(train_test_wells_file_path)\n",
    "print(f\"sc_profile shape: {sc_profile.shape}\")\n",
    "print(f\"sc_endpoint_profile shape: {sc_endpoint_profile.shape}\")\n",
    "data_split_file_path = pathlib.Path(\"../results/data_splits.parquet\").resolve()\n",
    "data_splits = pd.read_parquet(data_split_file_path)\n",
    "sc_profile[\"Metadata_Time\"] = sc_profile[\"Metadata_Time\"].astype(float).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80261090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop NaN rows\n",
    "# before_shape = sc_profile.shape\n",
    "# print(f\"sc_profile shape before dropping NaNs: {before_shape}\")\n",
    "# sc_profile = sc_profile.dropna()\n",
    "# print(f\"sc_profile shape after dropping NaNs: {sc_profile.shape}\")\n",
    "# print(f\"Dropped {before_shape[0] - sc_profile.shape[0]} rows with NaNs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a8b200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc_endpoint_profile_before_shape = sc_endpoint_profile.shape\n",
    "# print(f\"sc_endpoint_profile shape before dropping NaNs: {sc_endpoint_profile_before_shape}\")\n",
    "# sc_endpoint_profile = sc_endpoint_profile.dropna()\n",
    "# print(f\"sc_endpoint_profile shape after dropping NaNs: {sc_endpoint_profile.shape}\")\n",
    "# print(f\"Dropped {sc_endpoint_profile_before_shape[0] - sc_endpoint_profile.shape[0]} rows with NaNs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76faeccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_splits shape before filtering: (185502, 3)\n",
      "data_splits shape after filtering: (185502, 3)\n"
     ]
    }
   ],
   "source": [
    "# remove any rows of index that are not in sc_profile\n",
    "print(f\"data_splits shape before filtering: {data_splits.shape}\")\n",
    "data_splits = data_splits[data_splits[\"index\"].isin(sc_profile.index)]\n",
    "print(f\"data_splits shape after filtering: {data_splits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24c98a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the training data\n",
    "training_df_X = sc_profile.loc[\n",
    "    data_splits[\"index\"][data_splits[\"data_split\"] == \"train\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1d5ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_df_X =\n",
    "training_df_X = training_df_X.loc[\n",
    "    training_df_X[\"Metadata_Time\"] == training_df_X[\"Metadata_Time\"].max()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40ac1c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df_X[\"Metadata_Time\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60c914ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_df_X shape: (2336, 2383) 2336\n",
      "training_df_y shape: (2336, 545) 2336\n"
     ]
    }
   ],
   "source": [
    "training_df_y = sc_endpoint_profile.loc[\n",
    "    sc_endpoint_profile[\"Metadata_sc_unique_track_id\"].isin(\n",
    "        training_df_X[\"Metadata_sc_unique_track_id\"]\n",
    "    )\n",
    "]\n",
    "print(\n",
    "    f\"training_df_X shape: {training_df_X.shape}\",\n",
    "    training_df_X[\"Metadata_sc_unique_track_id\"].nunique(),\n",
    ")\n",
    "print(\n",
    "    f\"training_df_y shape: {training_df_y.shape}\",\n",
    "    training_df_y[\"Metadata_sc_unique_track_id\"].nunique(),\n",
    ")\n",
    "assert (\n",
    "    training_df_X[\"Metadata_sc_unique_track_id\"].nunique()\n",
    "    == training_df_y[\"Metadata_sc_unique_track_id\"].nunique()\n",
    ")\n",
    "assert (\n",
    "    training_df_X[\"Metadata_sc_unique_track_id\"].shape[0]\n",
    "    == training_df_y[\"Metadata_sc_unique_track_id\"].shape[0]\n",
    ")\n",
    "training_df_X_shuffled = training_df_X.copy()\n",
    "training_df_X_shuffled = shuffle_data(training_df_X_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6ac2086",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_metadata = [x for x in training_df_X.columns if \"Metadata\" in x]\n",
    "train_y_metadata = [y for y in training_df_y.columns if \"Metadata\" in y]\n",
    "training_X_features = [x for x in training_df_X.columns if x not in train_x_metadata]\n",
    "training_y_features = [y for y in training_df_y.columns if y not in train_y_metadata]\n",
    "\n",
    "train_x_shuffled_metadata = [\n",
    "    x for x in training_df_X_shuffled.columns if \"Metadata\" in x\n",
    "]\n",
    "train_y_shuffled_metadata = [y for y in training_df_y.columns if \"Metadata\" in y]\n",
    "train_x_shuffled_features = [\n",
    "    x for x in training_df_X_shuffled.columns if x not in train_x_shuffled_metadata\n",
    "]\n",
    "\n",
    "train_df_x_metadata = training_df_X[train_x_metadata]\n",
    "train_df_y_metadata = training_df_y[train_y_metadata]\n",
    "train_df_x_features = training_df_X[training_X_features]\n",
    "train_df_y_features = training_df_y[training_y_features]\n",
    "train_df_x_shuffled_metadata = training_df_X_shuffled[train_x_shuffled_metadata]\n",
    "train_df_x_shuffled_features = training_df_X_shuffled[train_x_shuffled_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a9a513e",
   "metadata": {},
   "outputs": [],
   "source": [
    "annexin_feature = \"Cytoplasm_Intensity_IntegratedIntensity_AnnexinV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af9682f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cross-validation strategy\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=0)  # 5-fold cross-validation\n",
    "# elastic net parameters\n",
    "elastic_net_params = {\n",
    "    \"alpha\": [0.1, 1.0, 10.0, 100.0, 1000.0],  # Regularization strength\n",
    "    \"l1_ratio\": [0.1, 0.25, 0.5, 0.75, 1.0],  # l1_ratio = 1.0 is Lasso\n",
    "    \"max_iter\": 10000,  # Increase max_iter for convergence\n",
    "}\n",
    "elastic_net_all_annexinv_features_model = MultiOutputRegressor(\n",
    "    ElasticNetCV(\n",
    "        alphas=elastic_net_params[\"alpha\"],\n",
    "        l1_ratio=elastic_net_params[\"l1_ratio\"],\n",
    "        cv=cv,\n",
    "        random_state=0,\n",
    "        max_iter=elastic_net_params[\"max_iter\"],\n",
    "    )\n",
    ")\n",
    "elastic_net_all_annexinv_features_model_shuffled = (\n",
    "    elastic_net_all_annexinv_features_model\n",
    ")\n",
    "elastic_net_single_terminal_features_model = ElasticNetCV(\n",
    "    alphas=elastic_net_params[\"alpha\"],\n",
    "    l1_ratio=elastic_net_params[\"l1_ratio\"],\n",
    "    cv=cv,\n",
    "    random_state=0,\n",
    "    max_iter=elastic_net_params[\"max_iter\"],\n",
    ")\n",
    "elastic_net_single_terminal_features_model_shuffled = (\n",
    "    elastic_net_single_terminal_features_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18d8ae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_train_tests = {\n",
    "    \"single_feature\": {\n",
    "        \"train\": {\n",
    "            \"X\": train_df_x_features,\n",
    "            \"y\": train_df_y_features[annexin_feature],\n",
    "            \"x_metadata\": train_df_x_metadata,\n",
    "            \"y_metadata\": train_df_y_metadata,\n",
    "            \"model\": elastic_net_single_terminal_features_model,\n",
    "            \"model_name\": \"elastic_net_single_terminal_features_model\",\n",
    "        },\n",
    "        \"train_shuffled\": {\n",
    "            \"X\": train_df_x_shuffled_features,\n",
    "            \"y\": train_df_y_features[annexin_feature],\n",
    "            \"x_metadata\": train_df_x_shuffled_metadata,\n",
    "            \"y_metadata\": train_df_y_metadata,\n",
    "            \"model\": elastic_net_single_terminal_features_model_shuffled,\n",
    "            \"model_name\": \"elastic_net_single_terminal_features_model_shuffled\",\n",
    "        },\n",
    "    },\n",
    "    \"annexinV_features\": {\n",
    "        \"train\": {\n",
    "            \"X\": train_df_x_features,\n",
    "            \"y\": train_df_y_features,\n",
    "            \"x_metadata\": train_df_x_metadata,\n",
    "            \"y_metadata\": train_df_y_metadata,\n",
    "            \"model\": elastic_net_all_annexinv_features_model,\n",
    "            \"model_name\": \"elastic_net_all_annexinv_features_model\",\n",
    "        },\n",
    "        \"train_shuffled\": {\n",
    "            \"X\": train_df_x_shuffled_features,\n",
    "            \"y\": train_df_y_features,\n",
    "            \"x_metadata\": train_df_x_shuffled_metadata,\n",
    "            \"y_metadata\": train_df_y_metadata,\n",
    "            \"model\": elastic_net_all_annexinv_features_model_shuffled,\n",
    "            \"model_name\": \"elastic_net_all_annexinv_features_model_shuffled\",\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef52aea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_y_features.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d44e011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for train...single_feature\n",
      "X shape: (2336, 2336), y shape: (2336,), x_metadata shape: (2336, 47), y_metadata shape: (2336, 35)\n",
      "Number of NaNs in X: 0, Number of NaNs in y: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:35<00:35, 35.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for train_shuffled...single_feature\n",
      "X shape: (2336, 2336), y shape: (2336,), x_metadata shape: (2336, 47), y_metadata shape: (2336, 35)\n",
      "Number of NaNs in X: 0, Number of NaNs in y: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:01<00:00, 30.75s/it]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for train...annexinV_features\n",
      "X shape: (2336, 2336), y shape: (2336, 510), x_metadata shape: (2336, 47), y_metadata shape: (2336, 35)\n",
      "Number of NaNs in X: 0, Number of NaNs in y: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [4:36:14<4:36:14, 16574.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for train_shuffled...annexinV_features\n",
      "X shape: (2336, 2336), y shape: (2336, 510), x_metadata shape: (2336, 47), y_metadata shape: (2336, 35)\n",
      "Number of NaNs in X: 0, Number of NaNs in y: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [6:55:10<00:00, 12455.11s/it]  \n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "for model_type in dict_of_train_tests.keys():\n",
    "    for train_test_key, train_test_data in tqdm.tqdm(\n",
    "        dict_of_train_tests[model_type].items()\n",
    "    ):\n",
    "        if \"test\" in train_test_key:\n",
    "            print(f\"Skipping {train_test_key} as it is a test set.\")\n",
    "            continue\n",
    "        print(f\"Training model for {train_test_key}...{model_type}\")\n",
    "        X = train_test_data[\"X\"]\n",
    "        y = train_test_data[\"y\"]\n",
    "        x_metadata = dict_of_train_tests[model_type][train_test_key][\"x_metadata\"]\n",
    "        y_metadata = dict_of_train_tests[model_type][train_test_key][\"y_metadata\"]\n",
    "        print(\n",
    "            f\"X shape: {X.shape}, y shape: {y.shape}, x_metadata shape: {x_metadata.shape}, y_metadata shape: {y_metadata.shape}\"\n",
    "        )\n",
    "        # find the number of NaNs\n",
    "        num_nans_X = X.isna().sum().sum()\n",
    "        num_nans_y = y.isna().sum().sum()\n",
    "        print(f\"Number of NaNs in X: {num_nans_X}, Number of NaNs in y: {num_nans_y}\")\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "            dict_of_train_tests[model_type][train_test_key][\"model\"].fit(X, y)\n",
    "\n",
    "        # save the model\n",
    "        model_path = (\n",
    "            model_dir / f\"{train_test_key}_{train_test_data['model_name']}.joblib\"\n",
    "        )\n",
    "        joblib.dump(train_test_data[\"model\"], model_path)\n",
    "        dict_of_train_tests[model_type][train_test_key][\"model_path\"] = model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4308ae9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 38657.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping train as it is a training set.\n",
      "Skipping train_shuffled as it is a training set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 28826.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping train as it is a training set.\n",
      "Skipping train_shuffled as it is a training set.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     43\u001b[39m         metrics_df[\u001b[33m\"\u001b[39m\u001b[33mtrain_test_key\u001b[39m\u001b[33m\"\u001b[39m] = train_test_key\n\u001b[32m     44\u001b[39m         metrics_df_list.append(metrics_df)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m metrics_df = pd.concat(metrics_df_list, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/timelapse_analysis_env/lib/python3.11/site-packages/pandas/core/reshape/concat.py:382\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[32m    380\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m op = _Concatenator(\n\u001b[32m    383\u001b[39m     objs,\n\u001b[32m    384\u001b[39m     axis=axis,\n\u001b[32m    385\u001b[39m     ignore_index=ignore_index,\n\u001b[32m    386\u001b[39m     join=join,\n\u001b[32m    387\u001b[39m     keys=keys,\n\u001b[32m    388\u001b[39m     levels=levels,\n\u001b[32m    389\u001b[39m     names=names,\n\u001b[32m    390\u001b[39m     verify_integrity=verify_integrity,\n\u001b[32m    391\u001b[39m     copy=copy,\n\u001b[32m    392\u001b[39m     sort=sort,\n\u001b[32m    393\u001b[39m )\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m op.get_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/timelapse_analysis_env/lib/python3.11/site-packages/pandas/core/reshape/concat.py:445\u001b[39m, in \u001b[36m_Concatenator.__init__\u001b[39m\u001b[34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28mself\u001b[39m.verify_integrity = verify_integrity\n\u001b[32m    443\u001b[39m \u001b[38;5;28mself\u001b[39m.copy = copy\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m objs, keys = \u001b[38;5;28mself\u001b[39m._clean_keys_and_objs(objs, keys)\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[32m    448\u001b[39m ndims = \u001b[38;5;28mself\u001b[39m._get_ndims(objs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/timelapse_analysis_env/lib/python3.11/site-packages/pandas/core/reshape/concat.py:507\u001b[39m, in \u001b[36m_Concatenator._clean_keys_and_objs\u001b[39m\u001b[34m(self, objs, keys)\u001b[39m\n\u001b[32m    504\u001b[39m     objs_list = \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[32m    506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo objects to concatenate\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    510\u001b[39m     objs_list = \u001b[38;5;28mlist\u001b[39m(com.not_none(*objs_list))\n",
      "\u001b[31mValueError\u001b[39m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "metrics_df_list = []\n",
    "# test the model\n",
    "for model_type in dict_of_train_tests.keys():\n",
    "    for train_test_key, train_test_data in tqdm.tqdm(\n",
    "        dict_of_train_tests[model_type].items()\n",
    "    ):\n",
    "        if \"train\" in train_test_key:\n",
    "            print(f\"Skipping {train_test_key} as it is a training set.\")\n",
    "            continue\n",
    "        print(model_type, train_test_key)\n",
    "        X = train_test_data[\"X\"]\n",
    "        y = train_test_data[\"y\"]\n",
    "        metadata = train_test_data[\"metadata\"]\n",
    "        if \"shuffled\" in train_test_key:\n",
    "            model_path = dict_of_train_tests[model_type][\"train_shuffled\"][\"model_path\"]\n",
    "        else:\n",
    "            model_path = dict_of_train_tests[model_type][\"train\"][\"model_path\"]\n",
    "\n",
    "        # load the model\n",
    "        model = joblib.load(model_path)\n",
    "\n",
    "        # make predictions\n",
    "        y_pred = model.predict(X)\n",
    "        if model_type == \"single_feature\":\n",
    "            model.alpha_\n",
    "            model.l1_ratio_\n",
    "        else:\n",
    "\n",
    "            alphas = model.estimators_[0].alpha_\n",
    "            l1_ratios = model.estimators_[0].l1_ratio_\n",
    "            print(f\"Model parameters for {train_test_key}:\")\n",
    "            print(f\"Alphas: {alphas}, L1 Ratios: {l1_ratios}\")\n",
    "\n",
    "        # calculate metrics\n",
    "        metrics = {\n",
    "            \"explained_variance\": explained_variance_score(y, y_pred),\n",
    "            \"mean_absolute_error\": mean_absolute_error(y, y_pred),\n",
    "            \"mean_squared_error\": mean_squared_error(y, y_pred),\n",
    "            \"r2_score\": r2_score(y, y_pred),\n",
    "        }\n",
    "        metrics_df = pd.DataFrame(metrics, index=[0])\n",
    "        metrics_df[\"model_type\"] = model_type\n",
    "        metrics_df[\"train_test_key\"] = train_test_key\n",
    "        metrics_df_list.append(metrics_df)\n",
    "metrics_df = pd.concat(metrics_df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b644e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a998a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot the metrics\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"model_type\", y=\"r2_score\", hue=\"train_test_key\", data=metrics_df)\n",
    "plt.title(\"R2 Score by Model Type and Data Split\")\n",
    "plt.ylabel(\"R2 Score\")\n",
    "plt.xlabel(\"Model Type\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(title=\"Data Split\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timelapse_analaysis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
